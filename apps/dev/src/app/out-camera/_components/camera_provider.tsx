"use client";

import React, {
  useCallback,
  useEffect,
  useMemo,
  useState,
  useRef,
} from "react";

import { WebGLCanvasCamera } from "@kyosan-map/out-camera/components/scalable-video";
import { useImageRecognizer } from "@kyosan-map/out-camera/hooks/recognizer-hook";
import { findNearestOCRBox } from "@kyosan-map/out-camera/functions/box_distance";
import { ImageActionProvider } from "@kyosan-map/out-camera/components/image-action-provider";
import type { OCRResult, Point } from "@kyosan-map/out-camera/types/type";

/**
 * ==========================================
 * å†…éƒ¨ã‚«ãƒ¡ãƒ©ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
 * ==========================================
 */
function CameraInner() {
  const recognizer = useImageRecognizer();
  const [stream, setStream] = useState<MediaStream | null>(null);
  const [isStarting, setIsStarting] = useState(false);
  const [isRunning, setIsRunning] = useState(false);
  const [lastResult, setLastResult] = useState<OCRResult[] | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  /** ğŸš€ ã‚«ãƒ¡ãƒ©é–‹å§‹ */
  const startCamera = useCallback(async () => {
    console.log("[startCamera] called");
    if (isStarting || isRunning) return;

    try {
      setIsStarting(true);
      const s = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode: { ideal: "environment" },
          width: { ideal: 1280 },
          height: { ideal: 720 },
        },
        audio: false,
      });

      console.log("[startCamera] stream obtained:", s);
      streamRef.current = s;
      setStream(s);
      setIsRunning(true);
    } catch (err) {
      console.error("[startCamera] failed:", err);
      alert("ã‚«ãƒ¡ãƒ©ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚");
    } finally {
      setIsStarting(false);
    }
  }, [isStarting, isRunning]);

  /** ğŸ›‘ ã‚«ãƒ¡ãƒ©åœæ­¢ */
  const stopCamera = useCallback(() => {
    console.log("[stopCamera] called");
    const s = streamRef.current;
    if (s) {
      s.getTracks().forEach((t) => t.stop());
      streamRef.current = null;
    }
    setStream(null);
    setIsRunning(false);
  }, []);

  /** ğŸš« unmountæ™‚ã‚‚ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ç¶­æŒï¼ˆiOS Safariå¯¾ç­–ï¼‰ */
  useEffect(() => {
    console.log("[CameraInner] mount");
    return () => {
      console.log("[CameraInner] unmount (stream preserved)");
      // ğŸ”¥ stream ã¯åœæ­¢ã—ãªã„
    };
  }, []);

  /** ğŸ‘† ã‚¿ãƒƒãƒ—æ™‚ã®OCRå‡¦ç† */
  const handleTap = useCallback(
    async (payload: { x: number; y: number; imageData: ImageData }) => {
      console.log("[handleTap] payload:", payload);

      if (!recognizer) {
        console.error("[handleTap] recognizer not ready");
        alert(
          "OCR ãŒã¾ã åˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        );
        return;
      }

      try {
        console.log("[handleTap] OCR start");
        const resultsRaw = await recognizer.run(payload.imageData);

        const results: OCRResult[] = resultsRaw[0]! as unknown as OCRResult[];
        console.log("[handleTap] OCR results:", results);
        setLastResult(results);

        if (!results || results.length === 0) {
          alert("æ–‡å­—ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚");
          return;
        }

        const tap: Point = [payload.x, payload.y];
        const nearest = findNearestOCRBox(tap, results);
        console.log("[handleTap] nearest:", nearest);

        if (!nearest) {
          alert("é©åˆ‡ãªé ˜åŸŸãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚");
          return;
        }

        const text = nearest.text.trim();
        const ok = window.confirm(`OCRçµæœã¯ã€Œ${text}ã€ã§ã™ã‹ï¼Ÿ`);
        alert(
          ok ? "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼" : "åˆ¥ã®é ˜åŸŸã‚’ã‚¿ãƒƒãƒ—ã—ã¦ã¿ã¦ãã ã•ã„ã€‚"
        );
      } catch (err) {
        console.error("[handleTap] error:", err);
        alert("OCR å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚");
      }
    },
    [recognizer]
  );

  // --------------------------------------
  // âœ… JSX
  // --------------------------------------
  return (
    <div className="w-full max-w-3xl mx-auto p-4">
      <div className="mb-4 flex items-center justify-between gap-2">
        <h1 className="text-xl font-semibold">Out-Camera OCR</h1>
        <div className="flex gap-2">
          <button
            onClick={startCamera}
            disabled={isStarting || isRunning}
            className={`px-4 py-2 rounded ${
              isStarting || isRunning
                ? "bg-gray-300 text-gray-500 cursor-not-allowed"
                : "bg-blue-600 text-white hover:bg-blue-700"
            }`}
          >
            {isStarting ? "èµ·å‹•ä¸­..." : "ã‚«ãƒ¡ãƒ©é–‹å§‹"}
          </button>
          <button
            onClick={stopCamera}
            disabled={!isRunning}
            className={`px-4 py-2 rounded ${
              !isRunning
                ? "bg-gray-300 text-gray-500 cursor-not-allowed"
                : "bg-rose-600 text-white hover:bg-rose-700"
            }`}
          >
            ã‚«ãƒ¡ãƒ©åœæ­¢
          </button>
        </div>
      </div>

      <div className="mb-3 text-sm text-gray-600">
        <div>Recognizer: {recognizer ? "âœ… ready" : "â³ loading..."}</div>
        <div>Camera: {isRunning ? "ğŸ“· èµ·å‹•ä¸­" : "â¸ åœæ­¢ä¸­"}</div>
      </div>

      {stream ? (
        <WebGLCanvasCamera
          stream={stream}
          width={500}
          height={500}
          onTap={handleTap}
          className="w-full h-auto"
        />
      ) : (
        <div className="flex items-center justify-center h-48 text-gray-400">
          ã‚«ãƒ¡ãƒ©ã‚’é–‹å§‹ã—ã¦ãã ã•ã„
        </div>
      )}

      <div className="mt-4">
        <details className="bg-gray-50 rounded-md p-3">
          <summary className="cursor-pointer select-none">
            ãƒ‡ãƒãƒƒã‚°: ç›´è¿‘ã® OCR çµæœ
          </summary>
          <pre className="mt-2 text-xs whitespace-pre-wrap break-words">
            {lastResult
              ? JSON.stringify(lastResult, null, 2)
              : "ï¼ˆã¾ã ã‚ã‚Šã¾ã›ã‚“ï¼‰"}
          </pre>
        </details>
      </div>
    </div>
  );
}

/**
 * ==========================================
 * è¦ªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆProviderã‚’ç¶­æŒï¼‰
 * ==========================================
 */
export function CameraProvider() {
  console.log("[CameraProvider] mount");

  const modelPaths = useMemo(
    () => ({
      det_model_path: "https://ocr-file-server.pages.dev/ppocrv5/det/det.ort",
      cls_model_path: "https://ocr-file-server.pages.dev/ppocrv5/cls/cls.ort",
      rec_model_path: "https://ocr-file-server.pages.dev/ppocrv5/rec/rec.ort",
      rec_char_dict_path:
        "https://ocr-file-server.pages.dev/ppocrv5/ppocrv5_dict.txt",
    }),
    []
  );
  const onnx_wasm_path = useMemo(
    () => "https://ocr-file-server.pages.dev/ort/",
    []
  );

  return (
    <ImageActionProvider
      modelPaths={modelPaths}
      onnx_wasm_path={onnx_wasm_path}
      loadingComponent={
        <div className="flex items-center justify-center min-h-[60vh]">
          <div className="text-center">
            <div className="inline-block animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600 mb-4" />
            <div className="text-gray-700">
              OCR ã‚’åˆæœŸåŒ–ä¸­ã§ã™â€¦ï¼ˆåˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰
            </div>
          </div>
        </div>
      }
      errorComponent={(error) => (
        <div className="flex items-center justify-center min-h-[60vh]">
          <div className="max-w-md p-4 bg-white rounded-lg shadow border">
            <h2 className="text-lg font-semibold text-rose-600 mb-2">
              åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼
            </h2>
            <p className="text-gray-700 mb-3">{error.message}</p>
            <button
              className="px-3 py-2 bg-rose-600 text-white rounded hover:bg-rose-700"
              onClick={() => window.location.reload()}
            >
              ãƒªãƒ­ãƒ¼ãƒ‰
            </button>
          </div>
        </div>
      )}
    >
      {/* âœ… Provider å†…ã«æ®‹ã™ */}
      <CameraInner />
    </ImageActionProvider>
  );
}
